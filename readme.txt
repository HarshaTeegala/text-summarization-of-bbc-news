Name: Lakshmi Sai Sree Harsha Teegala
Title : Text Summarizations Of BBC news


Multi document summarizing can be used by news organizations to group and summarize information from many sources. The goal of my project is to summarize news articles of BBC news from different domains using Extractive method and seq2seq. 

Four hundred seventeen political news stories from the BBC from 2004 to 2005 are included in this dataset for extractive text summarization in the News Articles folder. The Summaries folder has five summaries for each article. The title of each article appears in the first clause. This dataset was made using a dataset for data classification from the 2004â€“2005 work by D. Greene and P. Cunningham, which consisted of 2225 documents from the BBC news website relating to stories in five thematic categories. The BBC owns all rights, including copyright, in the material of the original papers published in "Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering", Proc. ICML 2006.4

I acquired this dataset from Kaggle. It has 2 directories. One for the original text files for articles which includes articles from business, entertainment, politics, sports and tech domain. Similarly, second directory is for Samples of the respective article text files from all sectors. There are total 4450 files.

The whole project was divided into multiple tasks:

A.	Analyze the dataset: I investigated the text files from my dataset that is BBC news summary and their respective summaries. The different sectors of news namely business, politics, etc had approx. 400-500 text files. This was a large dataset and hence a powerful algorithm is needed for test summarisation purpose. I studied the existing methods and read research papers.
B.	Loading the Libraries and Data: The libraries used were numpy, pandas, NLTK, matplotlib, TensorFlow, sklearn, OS, glob, TQDM, etc. I loaded all the text files using the OS, TQDM and glob libraries and created a function to read the articles text files and summaries text file separately from the path of the folder.
C.	Text cleaning and pre-processing: Prior to moving on to the model development phase, it is crucial to complete some fundamental pre-processing processes. Using sloppy and filthy text data might be a very bad idea. As a result, we will remove all extraneous symbols, letters, etc. from the text in this stage that have no bearing on the problem's main goal. In this step I removed stop words from the text files of articles. Converting them to Dataframe and displaying text files and their respective summaries using range function. I also used contraction mapping and cleaned the text. As Seq2seq model will be used. I added <START> and <END> tokens for the target text that is on the summary files.
D.	Understanding the distribution of the sequences: Here, I examined the length of the reviews and the summary to obtain a general sense of how long the material is distributed. This will enable to fix the sequence's maximum length. Additionally, I created Histograms to visualize the distribution of the articles and summaries. It was shown that the maximum length of the article was around 500 and the maximum length of summary was 100
E.	Splitting of the dataset: I split the data into training and testing modules.
F.	Tokenizer: A tokenizer increases vocabulary while transforming word sequences into numeric sequences. From tensorflow.keras.processing.text I imported tokenizer to create separate tokenisation function for articles text and summaries text.
G.	Building the Model: Finally, the section where we create the model is here. LSTM generates the hidden state and cell state for each timestep when the return sequences parameter is set to True. State = True on return: When return state is set to true, the LSTM only generates the hidden state and cell state from the previous timestep. Using Initial state, the LSTM's internal states are initialized for the first time. Layered LSTM consists of many LSTM layers stacked on top of one another. This results in a more accurate depiction of the scenario. As the loss function, I am using sparse categorical cross-entropy, which instantly transforms the integer sequence into a one-hot vector. This gets around any memory problems.6  Moreover, I am also monitoring validation loss value.
H.	Set up the Inference for Encoder and Decoder: After training, new source sequences with unknown target sequences are used to test the model. So, in order to decode a test sequence, we must configure the inference architecture.6
I.	Prediction: I describe the functions that turn a numeric sequence into a list of words for a generating the summary.

Initially, I proposed to use TextRank Algorithm for text summarization but after some attempts, for my dataset, in my opinion the algorithm was taking more time and was given multiple errors. Afterwards I discovered Seq2seq model which improved the methodology and showed me better results for the articles w.r.t the summaries. Tokenisation technique works good with text summarization. 

